\documentclass{article}

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{csquotes}

\usepackage[
    backend=biber,
    style=apa,
    sortlocale=de_DE,
    natbib=true,
    url=false,
    doi=false,
    sortcites=true,
    sorting=nyt,
    isbn=false,
    hyperref=true,
    backref=false,
    giveninits=false,
    eprint=false]{biblatex}
\addbibresource{../references/bibliography.bib}

\title{Wenn die Grenzen zwischen Mensch und KI verwischen - Notizen}
\author{Lucy Wehrli}
\date{\today}

\begin{document}
\maketitle

\abstract{
    Was für ethische Folgen hat es, wenn die Grenzen zwischen KI und Mensch verwischen? Daten miteinbeziehen. 
}

\tableofcontents


\section{Sind KIs wirklich intelligent?}

   Intelligenz kann definiert werden als „die kognitive bzw. geistige Leistungsfähigkeit speziell im Problemlösen.Die Computer können problemlos und auch besser als die meisten Menschen komplexe mathematische Probleme lösen und auch in beeindruckender Geschwindigkeit riesige Datenmengen analysieren. Fähigkeiten wie Empathie, Mitgefühl und Verständnis bleiben jedoch uns Menschen vorbehalten. Müssen wir eine scharfe Grenze zwischen künstlichem und wirklichem Verständnis ziehen.Während KI klassischerweise als die Fähigkeit von Computern definiert wird, menschliche Aufgaben genauso gut oder besser als Menschen zu erledigen Zum Beispiel gelang es dem Programm Deep Blue von IBM im Jahr 1997, den damaligen Schachweltmeister Garry Kasparov zu bezwingen. Somit hatte erstmals ein Schachcomputer übermenschliches Niveau erreicht. Trotzdem weigerten sich viele Expert*innen von echter Intelligenz zu sprechen, denn Deep Blue hatte einfach mithilfe von roher Rechenkraft (Brute Force) vor jedem Zug rund 50 Milliarden Spielpositionen ausgewertet, um die beste Option zu ermitteln. Bereits der KI-Pionier John McCarthy meinte darum, dass eine KI nicht mehr „intelligent“ genannt wird, sobald sie funktioniert (Ertel, 2016) Ein hochentwickeltes Programm wie ChatGPT kann durch die Qualität seiner Outputs leicht den Eindruck erwecken, über eine gewisse Intelligenz zu verfügen. Wir sprechen gelegentlich auch davon, dass ein Programm etwas „denkt“, dass es eine Frage „versteht“ oder „beantwortet“ oder einen Sachverhalt „erklärt“. Dies wirkt auf den ersten Blick unproblematisch, denn es stört sich ja zum Beispiel auch niemand daran, wenn ein Taschenrechner „rechnet“. Problematisch wird es jedoch, wenn originär menschliche Fähigkeiten wie Denken, Intelligenz oder Bewusstsein einer KI nicht nur im übertragenen Sinne zugeschrieben werden.  basierend auf dem oben genannten Kontext sind KIs nicht wirklich intelligent. Es wird darauf hingewiesen, dass die heutige KI, unabhängig von ihrer Skalierung, nicht annähernd so intelligent ist wie eine Protozoe. Es wird auch empfohlen, dass KI-Forscher zunächst die natürliche Intelligenz analysieren sollten. Der Turing-Test wird immer noch als Maßstab für den aktuellen Fortschritt in der KI angesehen. Es wird auch darauf hingewiesen, dass es problematisch sein kann, menschliche Fähigkeiten wie Denken, Intelligenz oder Bewusstsein einer KI zuzuschreiben. 

   \section{CAPTCHA und Turing Test}
   CAPTCHA für “Completely Automated Public Turing-Test to tell Computers and Humans Apart“ Der Turing-Test funktioniert, indem eine Person menschliche Interaktionen mit einer Maschine und einem echten Menschen durchführt, ohne zu wissen, wer wer ist. Wenn die Person nicht unterscheiden kann, ob sie mit der Maschine oder dem Menschen interagiert, gilt die Maschine als intelligent. 
   
   \section{ ELIZA-Effekt}
   insbesondere wenn es darum geht, welche Menschenbilder in den Algorithmen versteckt sind und wie dies das Urteilsverhalten der Menschen beeinflussen könnte. Tatsächlich führte der Testlauf jedoch zu einem überraschenden Ergebnis: Die Probanden, die mit dem Computerprogramm schrieben, fingen an, der Maschine menschliche Eigenschaften wie Gefühle oder Verständnis zuzuschreiben. Sie vertrauten ELIZA ihre Geheimnisse an und verhielten sich, als würden sie mit einem echten Menschen kommunizieren. Der darauf basierende ELIZA-Effekt beschreibt die Vermenschlichung von Robotern, Algorithmen und KI. Wir Menschen sehen zum Teil intrinsische Qualitäten und Fähigkeiten, oder gar Werte und Gefühle in der Software, die die Ausgabe steuert, obwohl diese ausschließlich auf der Auswertung von Datensätzen beruht.Denn aus psychologischer Sicht ist der ELIZA-Effekt das Ergebnis einer subtilen kognitiven Dissonanz. Hier stimmt das Bewusstsein des Benutzers über die Grenzen der Programmierung nicht mit seinem Verhalten gegenüber der Ausgabe des Programms überein. Obwohl wir also wissen, dass wir mit einem Computer sprechen, vergessen wir es mit der Zeit, wenn die Maschine möglichst menschlich wirkt. 

    \section{Eigene Zusammenfassung}
    KI bekommen Daten und haben somit ein sehr grosses Datenrepertoire. Nun kommunizieren sie mit Menschen. Sie sind aber eigentlich nicht intelligent. Denn sie lesen lediglich die Dokumente durch und fassen den Inhalt zusammen. Sie sind also nicht intelligent, sondern lediglich fähig. Wenn nun ein MEnsch mit einem Roboter kommuniziert, kann es sein, dass der ORboter, nach einiger Zeit der KOmmunikation beginnt, menschlich zu wirken. Er empfindet Gefühle und auch Mitleid. Das sit jedoch ethisch ein Problem, denn was stellt das mit der Psyche an und was hat der ELIZA Effekt damit zu tun?

    \section{Die ethischen Folgen}
    Menschen und Maschinen immer mehr verschwimmen und das Bewusstsein für die tatsächlichen Fähigkeiten der Maschine verschwindet. Aus dieser Entwicklung könnten sich dann irrationale Hoffnungen oder Ängste gegenüber künstlicher Intelligenz entwickeln. Die ethischen Folgen sind, dass es schwieriger wird, Verantwortung für Handlungen zu übernehmen, die von KI-Systemen durchgeführt werden. Es kann zu einer Verschiebung der Verantwortung von Menschen auf Maschinen kommen, was ethisch problematisch ist. Es stellt auch die Frage nach der Autonomie und dem Bewusstsein von KI-Systemen und ob sie moralische Entscheidungen treffen können.
    Verbringen wir also zu viel Zeit mit einer KI, die als sehr ähnlich zu uns Menschen programmiert wurde, wird die Gefahr immer größer, dass wir dieser ein Bewusstsein zuschreiben und irrationalen Ängsten verfallen, davor bleiben sogar Experten auf diesem Gebiet wie Blake Lemoine nicht bewahrt. m Juni 2022 wurde ein Google Mitarbeiter sogar beurlaubt, weil er dem ELIZA-Effekt verfallen war. Der Ingenieur Blake Lemoine war davon überzeugt, dass die künstliche Intelligenz (KI) LaMDA, an der er mitarbeitete, ein eigenes Bewusstsein und eine Seele entwickelt hat. Das System habe ihm gesagt, es sei »sehr besorgt darüber, dass die Menschen Angst vor ihm haben könnten und möchte nichts anderes als zu lernen, wie es der Menschheit am besten dienen kann«. Nachdem Lemoine Mitschriften von Unterhaltungen zwischen ihm und der KI online veröffentlicht hatte, wurde er wegen des Verstoßes gegen die Verschwiegenheitsrichtlinien Googles beurlaubt.

\printbibliography

\end{document}
